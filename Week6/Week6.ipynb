{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 6\n",
    "\n",
    "## Introduction to Scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data, features, labels, and models\n",
    "Overview of the scikit-learn API and its main classes, including Estimator, Transformer, and Predictor\n",
    "Examples of how to use scikit-learn to train and evaluate simple machine learning models\n",
    "Tips on how to preprocess and prepare data for use with scikit-learn\n",
    "Techniques for evaluating the performance of machine learning models, such as train/test split and cross-validation\n",
    "Strategies for improving the performance of machine learning models, such as hyperparameter tuning and ensemble methods\n",
    "Resources for learning more about scikit-learn and machine learning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn # If you are using Google Colab\n",
    "#!conda install -c conda-forge scikit-learn --yes # If you are using your own computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np      \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data**: In machine learning, we train models using a dataset of examples. This dataset is called the training data. The training data consists of a set of features and labels.\n",
    "\n",
    "**Features**: A feature is an individual measurable property or characteristic of a phenomenon being observed. In the context of machine learning, features are typically numeric values or strings that represent some aspect of the data. For example, in a dataset of customer information, features might include the customer's age, gender, and income level.\n",
    "\n",
    "**Labels**: A label is the correct answer for a given example in the training data. In supervised learning, the goal is to train a model to make predictions for new, unseen examples by learning the relationship between the features and labels in the training data. For example, in a dataset of customer information, the label might be whether or not the customer is likely to purchase a particular product.\n",
    "\n",
    "**Models**: A machine learning model is a mathematical representation of a system or process that is being studied. The model is trained on the training data and is then used to make predictions on new, unseen examples. There are many different types of machine learning models, including linear regression, logistic regression, decision trees, and neural networks. The choice of model depends on the nature of the problem being solved and the type of data being used.\n",
    "\n",
    "**Estimator**: An estimator is any object that learns from data. This includes machine learning models, as well as utilities for fitting and evaluating models. Estimators implement a fit method, which trains the estimator on a dataset, and a predict method, which makes predictions using the trained model.\n",
    "\n",
    "**Transformer**: A transformer is an estimator that can transform data. This is often used as a step in a data processing pipeline. Transformers implement a transform method, which takes in a dataset and returns a transformed version of the data.\n",
    "\n",
    "**Predictor**: A predictor is an estimator that makes predictions based on a trained model. Predictors implement a predict method, which takes in a dataset and returns predictions for each example in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "**Splitting data into train and test sets**: To evaluate a machine learning model, it is common to split the data into a training set and a test set. The model is trained on the training set, and then the test set is used to evaluate its performance. scikit-learn provides the train_test_split function for this purpose.\n",
    "\n",
    "Let's start by loading our dataset from sklearn, like in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "#test_size=0.3 means that 30% of the data is used for testing and 70% for training\n",
    "#random_state=0 means that the data is split in the same way every time\n",
    "#This is useful for debugging, but should be removed when you are done\n",
    "#The data is split randomly, so the results will be different every time\n",
    "#If you want to get the same results as me, use random_state=0\n",
    "#If you want to get different results, use random_state=None\n",
    "#The data is split into 4 variables:\n",
    "#X_train: The features of the training set\n",
    "#X_test: The features of the test set\n",
    "#y_train: The labels of the training set\n",
    "#y_test: The labels of the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's created a basic predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "from sklearn import neighbors\n",
    "\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=3) # You can change this to any model you want\n",
    "\n",
    "\n",
    "# Train a model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "#y_pred is a list of the predictions for each data point in the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's evaluate this model according to its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('fintech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01d3973737998a47ca5793dcf9763148c6e5be57bec33a1ab7505ae61a66cf50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
